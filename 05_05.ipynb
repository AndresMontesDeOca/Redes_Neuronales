{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndresMontesDeOca/Redes_Neuronales/blob/main/05_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdWG2FEMDVST"
      },
      "source": [
        "# Regularización L1 y L2 - Predicción de Energía\n",
        "---\n",
        "**Montar la carpeta de Google Drive y definir constantes para trabajar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HIP8SbdLIkDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fab7d02-eda7-46dc-905b-c0f5fdc53628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ColabNotebook = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if ColabNotebook:\n",
        "    # monta G-drive en entorno COLAB\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')\n",
        "\n",
        "    # carpeta donde se encuentran archivos .py auxiliares\n",
        "    FUENTES_DIR = '/content/drive/MyDrive/Colab Notebooks/Redes_Neuronales/Fuentes/'\n",
        "    DATOS_DIR = '/content/drive/MyDrive/Colab Notebooks/Redes_Neuronales/Data/'      # carpeta donde se encuentran los datasets\n",
        "else:\n",
        "    # configuración para notebook con instalación LOCAL\n",
        "    FUENTES_DIR = '../Fuentes'         # carpeta donde se encuentran archivos .py auxiliares\n",
        "    DATOS_DIR   = '../Datos/' # carpeta donde se encuentran los datasets\n",
        "\n",
        "# agrega ruta de busqueda donde tenemos archivos .py\n",
        "import sys\n",
        "sys.path.append(FUENTES_DIR)\n",
        "\n",
        "################################################################\n",
        "def plot_history(history, start_epoch=0, metrics=None):\n",
        "    if isinstance(metrics, str):\n",
        "        metrics = [metrics]\n",
        "\n",
        "    if metrics is None:\n",
        "        metrics = [x for x in history.history.keys() if x[:4] != 'val_']\n",
        "\n",
        "    if len(metrics) == 0:\n",
        "        print('No metrics to display.')\n",
        "        return\n",
        "\n",
        "    # Get the epochs and filter them starting from start_epoch\n",
        "    x = history.epoch[start_epoch:]\n",
        "\n",
        "    rows = 1\n",
        "    cols = len(metrics)\n",
        "    count = 0\n",
        "\n",
        "    plt.figure(figsize=(12 * cols, 8))\n",
        "\n",
        "    for metric in sorted(metrics):\n",
        "        count += 1\n",
        "        plt.subplot(rows, cols, count)\n",
        "        plt.plot(x, history.history[metric][start_epoch:], label='Train')\n",
        "        val_metric = f'val_{metric}'\n",
        "        if val_metric in history.history.keys():\n",
        "            plt.plot(x, history.history[val_metric][start_epoch:], label='Validation')\n",
        "        plt.title(metric.capitalize())\n",
        "        plt.legend()\n",
        "    plt.show()\n",
        "################################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para crear y compilar el modelo\n",
        "def create_model(regularizer=None):\n",
        "    model = Sequential([\n",
        "        Dense(30, activation='relu', kernel_regularizer=regularizer, input_shape=(X_train.shape[1],)),\n",
        "        Dense(20, activation='relu', kernel_regularizer=regularizer),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=SGD(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "# # Definición de las funciones\n",
        "# ################################################################\n",
        "# import keras.backend as K\n",
        "\n",
        "# def r2_score(y_true, y_pred):\n",
        "#     SS_res =  K.sum(K.square(y_true - y_pred))\n",
        "#     SS_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "#     return 1 - SS_res/(SS_tot + K.epsilon())\n",
        "# #############################################################################\n",
        "# def compile_model(new_model, loss, optimizer):\n",
        "#     new_model.compile(optimizer=optimizer, loss=loss, metrics=['mae', r2_score])\n",
        "#     print(new_model.summary())\n",
        "#     return new_model\n",
        "# #############################################################################\n",
        "# def MyModel(loss, optimizer, input_dim, output_dim):\n",
        "#     new_model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "#         tf.keras.layers.Dense(output_dim)\n",
        "#     ])\n",
        "#     return compile_model(new_model, loss, optimizer)\n",
        "# #############################################################################\n",
        "# def MyCallbacks(patience):\n",
        "#     early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "#     return [early_stop]\n",
        "# #############################################################################"
      ],
      "metadata": {
        "id": "Y8CaYBJ4tBye"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l1, l2\n",
        "\n",
        "# Definir el tamaño del batch\n",
        "batch_size = 32\n",
        "epochs = 150\n",
        "\n",
        "# División de los datos en conjuntos de entrenamiento y validación\n",
        "X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_tmp, y_train_tmp, test_size=0.4, random_state=42)\n",
        "\n",
        "# Escalado de los datos\n",
        "data_scaler, target_scaler = StandardScaler(), StandardScaler()\n",
        "X_train_scaled = data_scaler.fit_transform(X_train)\n",
        "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
        "X_valid_scaled = data_scaler.transform(X_valid)\n",
        "X_test_scaled = data_scaler.transform(X_test)\n",
        "y_valid_scaled = target_scaler.transform(y_valid.values.reshape(-1, 1))\n",
        "y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1))\n",
        "\n",
        "# Definir la función para crear el modelo\n",
        "def create_model(regularizer=None):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=regularizer),\n",
        "        Dense(64, activation='relu', kernel_regularizer=regularizer),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
        "    return model\n",
        "\n",
        "# Entrenar y evaluar el modelo sin regularización\n",
        "start_time = time.time()\n",
        "model_no_reg = create_model()\n",
        "history_no_reg = model_no_reg.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid_scaled), verbose=0)\n",
        "time_no_reg = time.time() - start_time\n",
        "\n",
        "# Entrenar y evaluar el modelo con regularización L1\n",
        "start_time = time.time()\n",
        "model_l1 = create_model(regularizer=l1(0.01))\n",
        "history_l1 = model_l1.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid_scaled), verbose=0)\n",
        "time_l1 = time.time() - start_time\n",
        "\n",
        "# Entrenar y evaluar el modelo con regularización L2\n",
        "start_time = time.time()\n",
        "model_l2 = create_model(regularizer=l2(0.01))\n",
        "history_l2 = model_l2.fit(X_train_scaled, y_train_scaled, epochs=epochs, batch_size=batch_size, validation_data=(X_valid_scaled, y_valid_scaled), verbose=0)\n",
        "time_l2 = time.time() - start_time\n",
        "\n",
        "# Evaluar los modelos en el conjunto de prueba\n",
        "test_results_no_reg = model_no_reg.evaluate(X_test_scaled, y_test_scaled, verbose=1)\n",
        "test_results_l1 = model_l1.evaluate(X_test_scaled, y_test_scaled, verbose=1)\n",
        "test_results_l2 = model_l2.evaluate(X_test_scaled, y_test_scaled, verbose=1)\n",
        "\n",
        "# Crear un diccionario con los resultados\n",
        "results = {\n",
        "    'Model': ['No Regularization', 'L1 Regularization', 'L2 Regularization'],\n",
        "    'Training Time (s)': [time_no_reg, time_l1, time_l2],\n",
        "    'Test Loss': [test_results_no_reg[0], test_results_l1[0], test_results_l2[0]],\n",
        "    'Test MAE': [test_results_no_reg[1], test_results_l1[1], test_results_l2[1]]\n",
        "}\n",
        "\n",
        "# Convertir el diccionario en un DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Redondear los valores a dos decimales\n",
        "results_df = results_df.round(2)\n",
        "\n",
        "# Mostrar el DataFrame\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "yjBE7ipJs_wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj5GbHXwGP1j"
      },
      "source": [
        "### Carga del Dataset y selección de atributos numéricos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Um9iFfH5i0"
      },
      "source": [
        "### Preparación de Datos\n",
        "Selección de atributos. División en entrenamiento y prueba. Normalización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If9sz9OvGrXp"
      },
      "outputs": [],
      "source": [
        "atr_pred = 'PE'\n",
        "df_var = df.select_dtypes(include=[np.number]) # seleccion de atributos numericos del dataset\n",
        "df_var = df_var.drop(atr_pred, axis=1) # elimina atributo a predecir\n",
        "\n",
        "X = np.array(df_var)\n",
        "T = np.array(df[atr_pred]).reshape((-1,1))\n",
        "\n",
        "# Dividir el dataset en entrenamiento y prueba\n",
        "X_train, X_test, T_train, T_test = train_test_split(X, T, test_size=0.3, random_state=42)\n",
        "\n",
        "# normalizacion de datos de entrada\n",
        "data_scaler = StandardScaler()\n",
        "\n",
        "X_train = data_scaler.fit_transform(X_train)\n",
        "X_test = data_scaler.transform(X_test)\n",
        "\n",
        "# normalización de atributo a predecir\n",
        "target_scaler = StandardScaler()\n",
        "T_train = target_scaler.fit_transform(T_train)\n",
        "T_test = target_scaler.transform(T_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufa1Z79_OMeG"
      },
      "source": [
        "### Construccion del Modelo, entrenamiento y Evaluación\n",
        "Se utilizan 3 modelos: sin regularización, con regularización L1 y con regularización L2. La función **crear_y_entrenar_modelo** genera un modelo y retorna el modelo, el historial y tiempo del entrenamiento y el error cuadrático medio sobre los datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "run_control": {
          "marked": false
        },
        "id": "1NWrWyOVKUPI"
      },
      "outputs": [],
      "source": [
        "# funcion para simplificar la replica del regularizador\n",
        "# espera la clase del regularizador o None si no se requiere\n",
        "def crear_regulizador(regularizador):\n",
        "    return None if regularizador is None else regularizador(0.001)\n",
        "\n",
        "def crear_y_entrenar_modelo(regularizacion=None, epocas=150):\n",
        "\n",
        "    # Sección de construccion del modelo\n",
        "    activacion = 'relu'\n",
        "    modelo = keras.Sequential([\n",
        "        layers.Dense(30,\n",
        "                     input_dim=X.shape[1],\n",
        "                     activation=activacion,\n",
        "                     kernel_regularizer=crear_regulizador(regularizacion)\n",
        "                    ),\n",
        "        layers.Dense(20, activation=activacion,\n",
        "                     kernel_regularizer=crear_regulizador(regularizacion)),\n",
        "        layers.Dense(1, activation='linear')\n",
        "    ])\n",
        "\n",
        "    ########################################################\n",
        "    # completar: compilar modelo con sdg como optimizador y error cuadrático medio como funcion de pérdida\n",
        "\n",
        "\n",
        "    # Sección de entrenamiento del modelo\n",
        "    print('Entrenando -> ', end='')\n",
        "\n",
        "    segs = time.time() # tiempo de inicio en segundos\n",
        "\n",
        "    ########################################################\n",
        "    # completar: entrenar el modelo con 100 epocas, batch de 64 y validación de 40%\n",
        "    historial = # completar\n",
        "\n",
        "\n",
        "    segs = time.time()-segs # tiempo total en segundos\n",
        "\n",
        "    print('%.2f segundos' % segs)\n",
        "\n",
        "    ########################################################\n",
        "    # completar: calcular error cuadrático promedio y r2 score del modelo sobre datos de entrenamiento\n",
        "    mse = # completar\n",
        "    r2 = # completar\n",
        "\n",
        "    ########################################################\n",
        "    # calcular error cuadrático promedio y r2 score del modelo sobre datos de prueb\n",
        "    mse_test = # completar\n",
        "    r2_test = # completar\n",
        "\n",
        "    return (modelo, historial, segs, mse, mse_test, r2, r2_test)\n",
        "\n",
        "\n",
        "# Crear, compilar y entrenar el modelo\n",
        "(modelo, historial, tpo, mse, mse_test, r2, r2_test) = crear_y_entrenar_modelo()\n",
        "(modelo_l1, historial_l1, tpo_l1, mse_l1, mse_test_l1, r2_l1, r2_test_l1) = crear_y_entrenar_modelo(regularizers.l1)\n",
        "(modelo_l2, historial_l2, tpo_l2, mse_l2, mse_test_l2, r2_l2, r2_test_l2) = crear_y_entrenar_modelo(regularizers.l2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izLfHOekKUPI"
      },
      "source": [
        "### Gráficos de Evolución del Entrenamiento\n",
        "Grafica función de pérdida para entrenamiento y validación en una subfigura"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIkSmjTTKUPJ"
      },
      "outputs": [],
      "source": [
        "# history tiene el historial del entrenamiento, axs es una subfigura, y_lim son limites verticales para la subfigura\n",
        "def dibujar_entrenamiento(history, axs=None, y_lim=None):\n",
        "\n",
        "    # Pérdida\n",
        "    axs.plot(history.history['loss'], label='Entrenamiento')\n",
        "    axs.plot(history.history['val_loss'], label='Validación')\n",
        "    axs.set_xlabel('Época')\n",
        "    axs.set_ylim(y_lim)\n",
        "    tick_frec = 5\n",
        "    epocas = len(history.history['loss'])\n",
        "    axs.set_xticks(np.arange(0, epocas, tick_frec))\n",
        "    axs.set_xticklabels(np.arange(0, epocas, tick_frec))\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 5))\n",
        "axs[0].set_title('Evolución de función de pérdida')\n",
        "\n",
        "\n",
        "y_lim = np.max([historial.history['val_loss'],historial_l1.history['val_loss'],historial_l2.history['val_loss']])\n",
        "\n",
        "dibujar_entrenamiento(historial, axs[0], (0, y_lim))\n",
        "dibujar_entrenamiento(historial_l1, axs[1], (0, y_lim))\n",
        "dibujar_entrenamiento(historial_l2, axs[2], (0, y_lim))\n",
        "\n",
        "axs[0].legend()\n",
        "plt.tight_layout()  # Ajustar el tamaño de la figura para dejar espacio para la leyenda\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### En esta sección se obtienen los pesos para cada modelo y se grafican"
      ],
      "metadata": {
        "id": "F8Sgxe8Nn9H7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uluGHxMhKUPJ"
      },
      "outputs": [],
      "source": [
        "# pesos en un vector con los pesos del modelo, axs es una subfigura, y_lim son limites verticales para la subfigura\n",
        "def graficar_pesos(pesos, ax, y_lim=(-1,1)):\n",
        "    # Graficar los pesos\n",
        "    ax.bar(range(0,len(pesos)), pesos, color='b', alpha=0.7)\n",
        "    ax.set_xlabel('Índices de Pesos')\n",
        "    ax.set_ylabel('Valor de Pesos')\n",
        "    ax.set_title('Pesos de la primera capa')\n",
        "    ax.set_ylim(y_lim)\n",
        "\n",
        "\n",
        "# subfiguras para graficar los pesos de los modelos uno debajo de otro\n",
        "fig, axs = plt.subplots(3, 1, figsize=(20, 10))\n",
        "\n",
        "#layer_id = 0\n",
        "layer_id = 1\n",
        "# obtiene pesos de los modelos\n",
        "mod_w = modelo.layers[layer_id].get_weights()[0].flatten()\n",
        "ml1_w = modelo_l1.layers[layer_id].get_weights()[0].flatten()\n",
        "ml2_w = modelo_l2.layers[layer_id].get_weights()[0].flatten()\n",
        "\n",
        "\n",
        "y_lim = np.max([np.abs(mod_w),np.abs(ml1_w), np.abs(ml2_w)]) # mismo limite para todas las subfiguras\n",
        "\n",
        "graficar_pesos(mod_w, axs[0], (-y_lim, y_lim))\n",
        "graficar_pesos(ml1_w, axs[1], (-y_lim, y_lim))\n",
        "graficar_pesos(ml2_w, axs[2], (-y_lim, y_lim))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esta sección se muestran distintas estadísticas/metricas sobre el entrenamiento y los pesos del modelo"
      ],
      "metadata": {
        "id": "285pjv2nl7gq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUUcugDEKUPJ"
      },
      "outputs": [],
      "source": [
        "# Esta sección se muestran distintas estadísticas/metricas sobre el entrenamiento y los pesos de cada modelo\n",
        "\n",
        "print('Entrenamiento en segundos.....: %7.1f %7.1f %7.1f' % (tpo, tpo_l1, tpo_l2) )\n",
        "print('Suma absoluta de pesos........: %7.4f %7.4f %7.4f' % ( np.sum(np.abs(mod_w)), np.sum(np.abs(ml1_w)), np.sum(np.abs(ml2_w))) )\n",
        "###########################################################\n",
        "# imprimir otras metricas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preguntas\n",
        " - Cómo es el tiempo de entrenamiento respecto del modelo con regularización respecto de sin regularización. ¿Por qué?\n",
        "\n",
        " - Observando las primeras épocas de las gráficas de las 3 funciones de pérdida de los entrenamientos de los modelos, indique las diferencias y justifique.\n",
        "\n",
        " - Grafique los pesos de la última capa oculta y realice algunas métricas sobre los pesos (suma y promedio absolutos, desviación, etc.). Marque las diferencias entre los 3 modelos e indique porque se dan.\n",
        "\n",
        " - La regularización es una herramienta útil para prevenir problemas de sobreajuste. ¿Percibe sobreajuste en los modelos? ¿En que beneficia o perjudica la regularización?\n"
      ],
      "metadata": {
        "id": "C9BFoF6lk_0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pqkdqEmTlK73"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "varInspector": {
      "cols": {
        "lenName": "16",
        "lenType": "16",
        "lenVar": "50"
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}